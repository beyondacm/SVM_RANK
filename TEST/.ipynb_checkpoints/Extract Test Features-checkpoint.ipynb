{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.download()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import re\n",
    "import nltk\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features From the Test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Test = pd.read_csv('./Final_Test_Set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_content</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>answer_voted</th>\n",
       "      <th>question_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_agrees</th>\n",
       "      <th>user_edu</th>\n",
       "      <th>user_exp</th>\n",
       "      <th>user_followed</th>\n",
       "      <th>user_helped</th>\n",
       "      <th>user_interest</th>\n",
       "      <th>user_intro</th>\n",
       "      <th>user_recommends</th>\n",
       "      <th>user_saved</th>\n",
       "      <th>user_specialty</th>\n",
       "      <th>user_thanks</th>\n",
       "      <th>question_tags</th>\n",
       "      <th>question_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10020</th>\n",
       "      <td>yes a replaced knee is often more swollen than...</td>\n",
       "      <td>1433245</td>\n",
       "      <td>1</td>\n",
       "      <td>1000543</td>\n",
       "      <td>11168524</td>\n",
       "      <td>189</td>\n",
       "      <td>uc san francisco school of medicine  ca</td>\n",
       "      <td>24years</td>\n",
       "      <td>[11193858, 11828102, 12242507, 11202045, 11139...</td>\n",
       "      <td>177,202</td>\n",
       "      <td>total hip replacement  total knee replacement ...</td>\n",
       "      <td>i did my med school and residency at ucsf and ...</td>\n",
       "      <td>dr dearborn is an amazing doctor #nationaldoct...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>orthopedic reconstructive surgery</td>\n",
       "      <td>111</td>\n",
       "      <td>nsaids  surgery  knee replacement</td>\n",
       "      <td>will swelling after knee replacement surgery g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          answer_content  answer_id  \\\n",
       "10020  yes a replaced knee is often more swollen than...    1433245   \n",
       "\n",
       "       answer_voted  question_id   user_id user_agrees  \\\n",
       "10020             1      1000543  11168524         189   \n",
       "\n",
       "                                      user_edu user_exp  \\\n",
       "10020  uc san francisco school of medicine  ca  24years   \n",
       "\n",
       "                                           user_followed user_helped  \\\n",
       "10020  [11193858, 11828102, 12242507, 11202045, 11139...     177,202   \n",
       "\n",
       "                                           user_interest  \\\n",
       "10020  total hip replacement  total knee replacement ...   \n",
       "\n",
       "                                              user_intro  \\\n",
       "10020  i did my med school and residency at ucsf and ...   \n",
       "\n",
       "                                         user_recommends  user_saved  \\\n",
       "10020  dr dearborn is an amazing doctor #nationaldoct...         NaN   \n",
       "\n",
       "                          user_specialty user_thanks  \\\n",
       "10020  orthopedic reconstructive surgery         111   \n",
       "\n",
       "                           question_tags  \\\n",
       "10020  nsaids  surgery  knee replacement   \n",
       "\n",
       "                                        question_content  \n",
       "10020  will swelling after knee replacement surgery g...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start of the Test_Set\n",
    "Test.iloc[[10020]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_normal_features(path_in, path_out): \n",
    "    m = pd.read_csv(path_in)\n",
    "\n",
    "    # calculate the features\n",
    "    # question_id :\n",
    "    question_id = m['question_id']\n",
    "    question_id.to_csv(path_out+'/qid.txt')\n",
    "\n",
    "    # vote :\n",
    "    vote = m['answer_voted']\n",
    "    vote.to_csv(path_out+'/vote.txt')\n",
    "\n",
    "    # feature1 : length of the answer_content \n",
    "    #answer_content = m[['answer_content','user_id']]\n",
    "    answer_content = m['answer_content']\n",
    "    # f1 : length of the answer_content\n",
    "    f1 = answer_content.str.len()\n",
    "    f1.to_csv(path_out+'/f1.txt')\n",
    "    #print answer_content\n",
    "\n",
    "\n",
    "    # feature2 : length of the user_introduction\n",
    "    user_intro = m['user_intro']\n",
    "    #print user_intro\n",
    "    f2 = user_intro.str.len()\n",
    "    f2.to_csv(path_out+'/f2.txt')\n",
    "    #print f2\n",
    "\n",
    "\n",
    "    # feature3 : length of the question_content\n",
    "    question_content = m['question_content']\n",
    "    f3 = question_content.str.len()\n",
    "    #print f3\n",
    "    f3.to_csv(path_out+'/f3.txt')\n",
    "\n",
    "\n",
    "    # feature4: user's experience years\n",
    "    user_exp = m['user_exp']\n",
    "    f4 = m['user_exp']\n",
    "    f4 = f4.str.extract('([0-9]*)')\n",
    "    f4.to_csv(path_out+'/f4.txt')\n",
    "    #print f4\n",
    "\n",
    "    # feature5 : length of the user's edu\n",
    "    user_edu = m['user_edu']\n",
    "    f5 = user_edu.str.len()\n",
    "    f5.to_csv(path_out+'/f5.txt')\n",
    "    #print f5\n",
    "\n",
    "\n",
    "    # feature6 : number of topics of user's interest\n",
    "    user_interest = m['user_interest']\n",
    "    f6 = user_interest.str.len()\n",
    "    f6.to_csv(path_out+'/f6.txt')\n",
    "    #print f6\n",
    "\n",
    "\n",
    "    # feature7 : number of the people user followed\n",
    "    user_followed = m['user_followed']\n",
    "    f7 = user_followed.str.len()\n",
    "    f7.to_csv(path_out+'/f7.txt')\n",
    "    #print f7\n",
    "\n",
    "    # feature8 : number of the question tags\n",
    "    question_tags = m['question_tags']\n",
    "    f8 = question_tags.str.len()\n",
    "    f8.to_csv(path_out+'/f8.txt')\n",
    "    #print f8\n",
    "\n",
    "\n",
    "    # feature9 : number of saved lives\n",
    "    saved_lives = m['user_saved']\n",
    "    f9 = saved_lives\n",
    "    f9.to_csv(path_out+'/f9.txt')\n",
    "    #print f9\n",
    "\n",
    "\n",
    "    # feature10 : number of recieved thanks\n",
    "    recieved_thanks = m['user_thanks']\n",
    "    #recieved_thanks = str(recieved_thanks)\n",
    "    f10 = recieved_thanks.str.replace(',','')\n",
    "    f10.to_csv(path_out+'/f10.txt')\n",
    "    #print f10\n",
    "\n",
    "\n",
    "    # feature11 : number of agrees\n",
    "    recieved_agrees = m['user_agrees']\n",
    "    f11 = recieved_agrees.str.replace(',','')\n",
    "    f11.to_csv(path_out+'/f11.txt')\n",
    "    #print f11\n",
    "\n",
    "\n",
    "    # feature12 : nuber of helped people\n",
    "    helped = m['user_helped']\n",
    "    f12 = helped.str.replace(',','')\n",
    "    f12.to_csv(path_out+'/f12.txt')\n",
    "    #print f12\n",
    "\n",
    "\n",
    "    # feature13 : number of doctor recommend\n",
    "    recommend = m['user_recommends']\n",
    "    f13 = recommend.str.len()\n",
    "    f13.to_csv(path_out+'/f13.txt')\n",
    "    #print f13\n",
    "\n",
    "    # feature14 : total length of the recommends\n",
    "    # recommend_str = m['user_recommends']\n",
    "    # recommend_str = ','.join(str(v) for v in recommend_str)\n",
    "    # f14 = recommend_str.to_string\n",
    "    # print f14\n",
    "    #f14.to_csv('f14.txt')\n",
    "\n",
    "\n",
    "# Extracting for the TEXT FEATRUES\n",
    "def extract_text_features(path_in, path_lda):\n",
    "    \n",
    "    m = pd.read_csv(path_in)\n",
    "\n",
    "    # Process with the question_content\n",
    "    qc = m['question_content']\n",
    "    #qc = str_strip(qc)\n",
    "    #print qc\n",
    "    qc.to_csv(path_lda+'/question_content.txt', encoding='utf-8')\n",
    "\n",
    "    # Process with the answer_content\n",
    "    ac = m['answer_content']\n",
    "    #ac = str_strip(ac)\n",
    "    ac.to_csv(path_lda+'/answer_content.txt', encoding='utf-8')\n",
    "\n",
    "    # Process with the user_recommends\n",
    "    ur = m['user_recommends'].astype(str)\n",
    "    #ur = str_strip(ur)\n",
    "    ur.to_csv(path_lda+'/user_recommends.txt', encoding='utf-8')\n",
    "\n",
    "    # Process with question_tags\n",
    "    qt = m['question_tags'].astype(str)\n",
    "    #qt = str_strip(qt)\n",
    "    #print qt\n",
    "    qt.to_csv(path_lda+'/question_tag.txt', encoding='utf-8')\n",
    "\n",
    "    # Process with user_interest\n",
    "    ut = m['user_interest'].astype(str)\n",
    "    #ut = str_strip(ut)\n",
    "    ut.to_csv(path_lda+'/user_interest.txt', encoding='utf-8')\n",
    "\n",
    "    # Proecess with user_specialty\n",
    "    us = m['user_specialty'].astype(str)\n",
    "    #us = str_strip(us)\n",
    "    us.to_csv(path_lda+'/user_specialty.txt', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(fin, fout) :\n",
    "    for line in fin:\n",
    "        word_list = line.split(',')[1].strip().split(' ')\n",
    "        filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "        print filtered_words\n",
    "        fout.write(' '.join(filtered_words))\n",
    "        fout.write('\\n')\n",
    "\n",
    "# do text - preprocessing \n",
    "# get tokens / remove stopwords / do stemming \n",
    "def text_processing(FIN, FOUT) :\n",
    "    for line in FIN :\n",
    "        stemmed = []\n",
    "\n",
    "        # do some strip to get tokens \n",
    "        word_str = line.split(',')[1]\n",
    "        word_str = word_str.replace('u\\'', '\\'')\n",
    "        word_str = word_str.replace(\"'\", \"\")\n",
    "        word_str = re.sub(r\"\\W+\", \" \", word_str)\n",
    "        word_list = word_str.strip().split(' ')\n",
    "        \n",
    "        # remove the stop words from tokens \n",
    "        filtered = [word for word in word_list if word not in stopwords.words('english')]\n",
    "        # print filtered\n",
    "        # do stemming on the word \n",
    "        # for word in filtered :\n",
    "            # stemmed.append(PorterStemmer().stem(word))\n",
    "            # stemmed.append(SnowballStemmer(\"english\").stem(word))\n",
    "        #FOUT.write(' '.join(stemmed))\n",
    "        FOUT.write(' '.join(filtered))\n",
    "        FOUT.write('\\n')\n",
    "\n",
    "    return \n",
    "\n",
    "def process_text_features(PATH_IN, PATH_OUT) :\n",
    "    \n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding('utf8')\n",
    "    try :\n",
    "        with open(PATH_IN+'question_content.txt','r') as f_in1,        \\\n",
    "            open(PATH_OUT+'question_content_pro.txt','w') as f_out1,   \\\n",
    "            open(PATH_IN+'answer_content.txt','r') as f_in2,           \\\n",
    "            open(PATH_OUT+'answer_content_pro.txt','w') as f_out2,     \\\n",
    "            open(PATH_IN+'user_interest.txt','r') as f_in3,            \\\n",
    "            open(PATH_OUT+'user_interest_pro.txt','w') as f_out3,      \\\n",
    "            open(PATH_IN+'question_tag.txt','r') as f_in4,             \\\n",
    "            open(PATH_OUT+'question_tag_pro.txt','w') as f_out4,       \\\n",
    "            open(PATH_IN+'user_recommends.txt','r') as f_in5,          \\\n",
    "            open(PATH_OUT+'user_recommends_pro.txt', 'w') as f_out5,   \\\n",
    "            open(PATH_IN+'user_specialty.txt', 'r') as f_in6,          \\\n",
    "            open(PATH_OUT+'user_specialty_pro.txt', 'w') as f_out6:    \n",
    "\n",
    "                text_processing(f_in1, f_out1)\n",
    "                text_processing(f_in2, f_out2)\n",
    "                text_processing(f_in3, f_out3)\n",
    "                text_processing(f_in4, f_out4)\n",
    "                text_processing(f_in5, f_out5)\n",
    "                text_processing(f_in6, f_out6)\n",
    "    except IOError as e:\n",
    "        print 'Operation failed: %s' % e.strerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import izip\n",
    "\n",
    "def unique(a):\n",
    "    return list(set(a))\n",
    "\n",
    "def intersect(a,b):\n",
    "    return list(set(a)&set(b))\n",
    "\n",
    "def union(a,b):\n",
    "    return list(set(a) | set(b))\n",
    "\n",
    "# Run this code for best_answer_features\n",
    "# PATH_IN = 'DIVIDE/match/best_answer_features/OVER_LAP/'\n",
    "# PATH_OUT = 'DIVIDE/match/best_answer_features/OVER_LAP/'\n",
    "# PATH_FEATURES = 'DIVIDE/match/best_answer_features/'\n",
    "\n",
    "def extract_overlap_features(PATH_IN, PATH_OUT, PATH_FEATURES) :\n",
    "    fin_1 = open(PATH_IN + 'answer_content_pro.txt','r')\n",
    "    fin_2 = open(PATH_IN + 'question_content_pro.txt','r')\n",
    "    fin_3 = open(PATH_IN + 'user_interest_pro.txt')\n",
    "\n",
    "    fout_1 = open(PATH_OUT + 'q_a_overlap.txt','w')\n",
    "    fout_2 = open(PATH_OUT + 'q_u_overlap.txt','w')\n",
    "    fout_3 = open(PATH_OUT + 'a_u_overlap.txt','w')\n",
    "\n",
    "    f15 = open(PATH_FEATURES + 'f15.txt', 'w')\n",
    "    f16 = open(PATH_FEATURES + 'f16.txt', 'w')\n",
    "    f17 = open(PATH_FEATURES + 'f17.txt', 'w')\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for line1, line2, line3 in izip(fin_1, fin_2, fin_3):\n",
    "        word_list1 = line1.strip().split(' ')\n",
    "        word_list2 = line2.strip().split(' ')\n",
    "        word_list3 = line3.strip().split(' ')\n",
    "\n",
    "        over_lap1 = intersect(word_list1, word_list2)\n",
    "        over_lap2 = intersect(word_list1, word_list3)\n",
    "        over_lap3 = intersect(word_list2, word_list3)\n",
    "\n",
    "        len1 = len(over_lap1)\n",
    "        len2 = len(over_lap2)\n",
    "        len3 = len(over_lap3)\n",
    "\n",
    "        print over_lap1\n",
    "        fout_1.write(str(count)+',')\n",
    "        fout_1.write(' '.join(over_lap1))\n",
    "        fout_1.write(','+str(len1)+'\\n')\n",
    "\n",
    "        fout_2.write(str(count)+',')\n",
    "        fout_2.write(' '.join(over_lap2))\n",
    "        fout_2.write(','+str(len2)+'\\n')\n",
    "\n",
    "        fout_3.write(str(count)+',')\n",
    "        fout_3.write(' '.join(over_lap3))\n",
    "        fout_3.write(','+str(len3)+'\\n')\n",
    "\n",
    "\n",
    "        f15.write(str(count)+','+str(len1)+'\\n')\n",
    "        f16.write(str(count)+','+str(len2)+'\\n')\n",
    "        f17.write(str(count)+','+str(len3)+'\\n')\n",
    "\n",
    "        count = count + 1\n",
    "\n",
    "    fin_1.close()\n",
    "    fin_2.close()\n",
    "    fin_3.close()\n",
    "\n",
    "    fout_1.close()\n",
    "    fout_2.close()\n",
    "    fout_3.close()\n",
    "\n",
    "    f15.close()\n",
    "    f16.close()\n",
    "    f17.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_source(PATH_IN, PATH_OUT) :\n",
    "\n",
    "    f1 = open(PATH_IN + 'question_content_pro.txt','r')\n",
    "    f2 = open(PATH_IN + 'answer_content_pro.txt','r')\n",
    "    f3 = open(PATH_IN + 'user_interest_pro.txt', 'r')\n",
    "    f4 = open(PATH_IN + 'question_tag_pro.txt', 'r')\n",
    "\n",
    "    fout = open(PATH_OUT + 'TEST_SOURCE.txt','w')\n",
    "\n",
    "    for line1, line2, line3, line4 in izip(f1, f2, f3, f4) :\n",
    "        #line1 = f1.readline()\n",
    "        info1 = line1.strip()\n",
    "        print info1\n",
    "        fout.write(info1)\n",
    "\n",
    "        #line2 = f2.readline()\n",
    "        info2 = line2.strip()\n",
    "        print info2\n",
    "        fout.write(' '+info2)\n",
    "\n",
    "        info3 = line3.strip()\n",
    "        print info3\n",
    "        fout.write(' '+info3)\n",
    "\n",
    "        info4 = line4.strip()\n",
    "        print info4\n",
    "        fout.write(' '+info4 + '\\n')\n",
    "\n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    f3.close()\n",
    "    f4.close()\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(PATH_IN, PATH_OUT) :\n",
    "    # feature1 : length of the answer_content \n",
    "    f1 = pd.read_csv(PATH_IN + 'f1.txt', names = ['index','len_answer_content'])\n",
    "\n",
    "    # feature2 : length of the user_introduction\n",
    "    f2 = pd.read_csv(PATH_IN + 'f2.txt', names = ['index','len_user_intro'])\n",
    "    m = pd.merge(f1, f2, how='outer')\n",
    "    #print m\n",
    "\n",
    "    # feature3 : length of the question_content\n",
    "    f3 = pd.read_csv(PATH_IN + 'f3.txt', names = ['index', 'len_question_content'])\n",
    "    m = pd.merge(m, f3, how='outer')\n",
    "\n",
    "    # feature4: user's experience years\n",
    "    f4 = pd.read_csv(PATH_IN + 'f4.txt', names = ['index','user_exp'])\n",
    "    m = pd.merge(m, f4, how='outer')\n",
    "    #print m\n",
    "\n",
    "    # feature5 : length of the user's edu\n",
    "    f5 = pd.read_csv(PATH_IN + 'f5.txt', names = ['index','len_user_edu'])\n",
    "    m = pd.merge(m, f5, how='outer')\n",
    "\n",
    "    # feature6 : number of topics of user's interest\n",
    "    f6 = pd.read_csv(PATH_IN + 'f6.txt', names = ['index','num_user_interst'])\n",
    "    m = pd.merge(m, f6, how='outer')\n",
    "\n",
    "    # feature7 : number of the people user followed\n",
    "    f7 = pd.read_csv(PATH_IN + 'f7.txt', names = ['index','num_user_followed'])\n",
    "    m = pd.merge(m, f7, how='outer')\n",
    "\n",
    "    # feature8 : number of the question tags\n",
    "    f8 = pd.read_csv(PATH_IN + 'f8.txt', names = ['index', 'num_question_tags'])\n",
    "    m = pd.merge(m, f8, how='outer')\n",
    "\n",
    "    # feature9 : number of saved lives\n",
    "    f9 = pd.read_csv(PATH_IN + 'f9.txt', names = ['index','num_user_saved'])\n",
    "    m = pd.merge(m, f9, how='outer')\n",
    "\n",
    "    # feature10 : number of recieved thanks\n",
    "    f10 = pd.read_csv(PATH_IN + 'f10.txt', names = ['index','num_user_thanks'])\n",
    "    m = pd.merge(m, f10, how='outer')\n",
    "\n",
    "    # feature11 : number of agrees\n",
    "    f11 = pd.read_csv(PATH_IN + 'f11.txt', names = ['index','num_user_agrees'])\n",
    "    m = pd.merge(m, f11, how='outer')\n",
    "\n",
    "    # feature12 : nuber of helped people\n",
    "    f12 = pd.read_csv(PATH_IN + 'f12.txt', names = ['index','num_user_helped'])\n",
    "    m = pd.merge(m, f12, how='outer')\n",
    "\n",
    "    # feature13 : number of doctor recommend\n",
    "    f13 = pd.read_csv(PATH_IN + 'f13.txt', names = ['index','num_user_recommend'])\n",
    "    m = pd.merge(m, f13, how='outer')\n",
    "\n",
    "    #####################################################################\n",
    "    # feature15 : number of answer_question overlap\n",
    "    f15 = pd.read_csv(PATH_IN + 'f15.txt', names = ['index', 'answer_question_overlap'])\n",
    "    m = pd.merge(m, f15, how='outer')\n",
    "\n",
    "\n",
    "    # feature16 : number of question_user_interest overlap\n",
    "    f16 = pd.read_csv(PATH_IN + 'f16.txt', names = ['index', 'question_uinterest_overlap'])\n",
    "    m = pd.merge(m, f16, how= 'outer')\n",
    "\n",
    "    # feature17 : number of user_interest_answer overlap\n",
    "    f17 = pd.read_csv(PATH_IN + 'f17.txt', names = ['index', 'uinterest_answer_overlap'])\n",
    "    m = pd.merge(m, f17, how= 'outer')\n",
    "\n",
    "    #######################################################################\n",
    "    Index = m.ix[:,0] \n",
    "    M = m.ix[:,1:]\n",
    "\n",
    "    M = (M - M.min()) / (M.max() - M.min())\n",
    "    M['index'] = Index\n",
    "\n",
    "\n",
    "    # question_id :\n",
    "    # qid = pd.read_csv(PATH_IN + 'qid.txt', names = ['index', 'question_id'])\n",
    "    # M = pd.merge(M, qid, how = 'outer')\n",
    "\n",
    "    # number_of_vote\n",
    "    # vote = pd.read_csv(PATH_IN + 'vote.txt', names = ['index', 'vote'])\n",
    "    # M = pd.merge(M, vote, how = 'outer')\n",
    "\n",
    "    M = M.drop('index', axis = 1)\n",
    "    M = M.fillna(0)\n",
    "    # print M\n",
    "\n",
    "    # M.to_csv(PATH_OUT + 'Normal_Features.txt',  index = False)\n",
    "    M.to_csv(PATH_OUT + 'Total_Normal.txt',  index = False)\n",
    "    \n",
    "    '''\n",
    "    m_norm = (m-m.mean()) / (m.max() - m.min())\n",
    "    print m_norm\n",
    "\n",
    "    print m.max()\n",
    "    print m.min()\n",
    "    print m.mean()\n",
    "    #print m.std()\n",
    "    '''\n",
    "\n",
    "    #M_norm.to_csv('Normaliztion.txt',  index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_IN = './Final_Test_Set.csv'\n",
    "PATH_NORMAL = './NORMAL/'\n",
    "PATH_OVERLAP = './OVERLAP/'\n",
    "PATH_TEXT = './TEXT/'\n",
    "PATH_LDA = './LDA/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_normal_features(PATH_IN, PATH_NORMAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_text_features(PATH_IN, PATH_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_text_features(PATH_TEXT, PATH_OVERLAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract_overlap_features(PATH_OVERLAP, PATH_OVERLAP, PATH_NORMAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_source(PATH_OVERLAP, PATH_LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "normalize(PATH_NORMAL, PATH_NORMAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport os\\n\\ndata_path = './LDA/'\\nwith open('./LDA/LDA_SOURCE.txt', 'w') as outfile:\\n    for fname in os.listdir(data_path):\\n        if fname[0] == '.':\\n            continue\\n        path = os.path.join(data_path,fname)\\n        #print path\\n\\n        with open(path,'r') as infile :\\n            outfile.write(infile.read())\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "\n",
    "data_path = './LDA/'\n",
    "with open('./LDA/LDA_SOURCE.txt', 'w') as outfile:\n",
    "    for fname in os.listdir(data_path):\n",
    "        if fname[0] == '.':\n",
    "            continue\n",
    "        path = os.path.join(data_path,fname)\n",
    "        #print path\n",
    "\n",
    "        with open(path,'r') as infile :\n",
    "            outfile.write(infile.read())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Format the S2V\n",
    "s2v_list = []\n",
    "for i in range(50) :\n",
    "    col_name = 's2v_' + str(i) \n",
    "    s2v_list.append(col_name)\n",
    "# print s2v_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !!! before this , you should remove the first row of the Test_Source.txt.vec\n",
    "total_s2v = pd.read_table('./SENTENCE_VECTOR/TEST_SOURCE.txt.vec', sep = ' ', names = s2v_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_s2v !!! very important : Normalization\n",
    "total_s2v = (total_s2v - total_s2v.min()) / (total_s2v.max() - total_s2v.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total_s2v # 11292 rows Ã— 50 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_s2v.to_csv('./SENTENCE_VECTOR/Total_S2V.csv', encoding='utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# T_s2v\n",
    "topic_list = []\n",
    "for i in range(50) :\n",
    "    col_name = 'topic_' + str(i) \n",
    "    topic_list.append(col_name)\n",
    "# print topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_topic = pd.read_table('./LDA/model-final.theta', sep = ' ', names = topic_list, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_topic !!! very important : Normalization\n",
    "test_topic = (test_topic - test_topic.min()) / (test_topic.max() - test_topic.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_topic = test_topic[10020:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_topic\n",
    "test_topic.to_csv('./LDA/Total_Topic.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# T_topic = pd.read_csv('TEST/LDA/test_topic.csv')\n",
    "# T_topic\n",
    "\n",
    "Total_Normal = pd.read_csv('./NORMAL/Total_Normal.txt')\n",
    "Total_S2V = pd.read_csv('./SENTENCE_VECTOR/Total_S2V.csv')\n",
    "Total_Topic = pd.read_csv('./LDA/Total_Topic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_normal\n",
    "# test_s2v\n",
    "# test_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Total_Normal_S2V = pd.merge(Total_Normal, Total_S2V, left_index=True, right_index=True)\n",
    "Total_Final = pd.merge(Total_Normal_S2V, Total_Topic, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Total_Final # 11921 x 116 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Total_Final.to_csv('./Total_Final_Feature.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Total_Final  # 11292 x 116 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_len = [10, 8, 6, 6, 10, 6, 7, 6, 6, 7, 8, 8, 6, 8, 8, 6, 6, 7, 5, 7, 6, 6, 8, 7, 6, 8, 7, 6, 7, 6, 8, 6, 5, 6, 9, 7, 7, 6, 7, 5, 6, 6, 5, 5, 6, 7, 7, 7, 5, 8, 7, 8, 5, 5, 6, 5, 7, 6, 7, 7, 8, 9, 7, 8, 7, 7, 8, 7, 6, 7, 7, 5, 6, 7, 6, 9, 9, 5, 6, 5, 6, 6, 7, 7, 5, 5, 6, 5, 7, 5, 7, 10, 6, 7, 6, 6, 6, 6, 6, 5, 7, 7, 6, 5, 5, 6, 6, 9, 8, 7, 6, 7, 8, 7, 8, 7, 6, 7, 8, 5, 7, 5, 5, 6, 9, 7, 7, 7, 9, 7, 7, 7, 5, 7, 5, 6, 8, 5, 7, 6, 5, 6, 6, 6, 5, 5, 6, 6, 6, 6, 6, 5, 6, 5, 7, 9, 6, 7, 7, 7, 6, 5, 7, 7, 7, 7, 8, 8, 7, 5, 6, 7, 7, 7, 9, 8, 7, 8, 8, 8, 6, 6, 6, 5, 7, 5, 7, 6, 5, 6, 7, 6, 7, 9, 6, 8, 9, 7, 7, 5, 5, 6, 5, 7, 6, 5, 6, 6, 8, 5, 8, 7, 6, 7, 6, 7, 8, 7, 7, 6, 7, 8, 6, 8, 7, 7, 8, 9, 6, 7, 5, 7, 7, 7, 7, 6, 5, 7, 6, 6, 9, 8, 6, 9, 7, 6, 6, 10, 8, 5, 6, 10, 7, 7, 5, 8, 8, 8, 6, 6, 8, 6, 6, 7, 7, 6, 6, 10, 6, 5, 7, 9, 5, 8, 9, 8, 7, 8, 6, 5, 6, 5, 6, 6, 10, 5, 6, 5, 6, 5, 7, 8, 6, 6, 5, 6, 5, 6, 7, 6, 5, 6, 7, 5, 6, 7, 6, 6, 7, 6, 9, 7, 8, 7, 7, 7, 6, 6, 8, 7, 6, 5, 6, 7, 7, 6, 5, 6, 6, 6, 6, 8, 8, 7, 6, 6, 9, 7, 6, 7, 7, 6, 7, 7, 7, 6, 8, 7, 8, 6, 8, 7, 7, 7, 5, 7, 5, 7, 5, 7, 6, 7, 8, 7, 8, 6, 5, 6, 5, 10, 6, 6, 8, 8, 7, 7, 7, 7, 7, 5, 6, 7, 5, 5, 5, 7, 5, 6, 5, 5, 9, 7, 7, 6, 8, 7, 6, 6, 6, 5, 7, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(test_len) 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10020, 10030)\n",
      "(10030, 10038)\n",
      "(10038, 10044)\n",
      "(10044, 10050)\n",
      "(10050, 10060)\n",
      "(10060, 10066)\n",
      "(10066, 10073)\n",
      "(10073, 10079)\n",
      "(10079, 10085)\n",
      "(10085, 10092)\n",
      "(10092, 10100)\n",
      "(10100, 10108)\n",
      "(10108, 10114)\n",
      "(10114, 10122)\n",
      "(10122, 10130)\n",
      "(10130, 10136)\n",
      "(10136, 10142)\n",
      "(10142, 10149)\n",
      "(10149, 10154)\n",
      "(10154, 10161)\n",
      "(10161, 10167)\n",
      "(10167, 10173)\n",
      "(10173, 10181)\n",
      "(10181, 10188)\n",
      "(10188, 10194)\n",
      "(10194, 10202)\n",
      "(10202, 10209)\n",
      "(10209, 10215)\n",
      "(10215, 10222)\n",
      "(10222, 10228)\n",
      "(10228, 10236)\n",
      "(10236, 10242)\n",
      "(10242, 10247)\n",
      "(10247, 10253)\n",
      "(10253, 10262)\n",
      "(10262, 10269)\n",
      "(10269, 10276)\n",
      "(10276, 10282)\n",
      "(10282, 10289)\n",
      "(10289, 10294)\n",
      "(10294, 10300)\n",
      "(10300, 10306)\n",
      "(10306, 10311)\n",
      "(10311, 10316)\n",
      "(10316, 10322)\n",
      "(10322, 10329)\n",
      "(10329, 10336)\n",
      "(10336, 10343)\n",
      "(10343, 10348)\n",
      "(10348, 10356)\n",
      "(10356, 10363)\n",
      "(10363, 10371)\n",
      "(10371, 10376)\n",
      "(10376, 10381)\n",
      "(10381, 10387)\n",
      "(10387, 10392)\n",
      "(10392, 10399)\n",
      "(10399, 10405)\n",
      "(10405, 10412)\n",
      "(10412, 10419)\n",
      "(10419, 10427)\n",
      "(10427, 10436)\n",
      "(10436, 10443)\n",
      "(10443, 10451)\n",
      "(10451, 10458)\n",
      "(10458, 10465)\n",
      "(10465, 10473)\n",
      "(10473, 10480)\n",
      "(10480, 10486)\n",
      "(10486, 10493)\n",
      "(10493, 10500)\n",
      "(10500, 10505)\n",
      "(10505, 10511)\n",
      "(10511, 10518)\n",
      "(10518, 10524)\n",
      "(10524, 10533)\n",
      "(10533, 10542)\n",
      "(10542, 10547)\n",
      "(10547, 10553)\n",
      "(10553, 10558)\n",
      "(10558, 10564)\n",
      "(10564, 10570)\n",
      "(10570, 10577)\n",
      "(10577, 10584)\n",
      "(10584, 10589)\n",
      "(10589, 10594)\n",
      "(10594, 10600)\n",
      "(10600, 10605)\n",
      "(10605, 10612)\n",
      "(10612, 10617)\n",
      "(10617, 10624)\n",
      "(10624, 10634)\n",
      "(10634, 10640)\n",
      "(10640, 10647)\n",
      "(10647, 10653)\n",
      "(10653, 10659)\n",
      "(10659, 10665)\n",
      "(10665, 10671)\n",
      "(10671, 10677)\n",
      "(10677, 10682)\n",
      "(10682, 10689)\n",
      "(10689, 10696)\n",
      "(10696, 10702)\n",
      "(10702, 10707)\n",
      "(10707, 10712)\n",
      "(10712, 10718)\n",
      "(10718, 10724)\n",
      "(10724, 10733)\n",
      "(10733, 10741)\n",
      "(10741, 10748)\n",
      "(10748, 10754)\n",
      "(10754, 10761)\n",
      "(10761, 10769)\n",
      "(10769, 10776)\n",
      "(10776, 10784)\n",
      "(10784, 10791)\n",
      "(10791, 10797)\n",
      "(10797, 10804)\n",
      "(10804, 10812)\n",
      "(10812, 10817)\n",
      "(10817, 10824)\n",
      "(10824, 10829)\n",
      "(10829, 10834)\n",
      "(10834, 10840)\n",
      "(10840, 10849)\n",
      "(10849, 10856)\n",
      "(10856, 10863)\n",
      "(10863, 10870)\n",
      "(10870, 10879)\n",
      "(10879, 10886)\n",
      "(10886, 10893)\n",
      "(10893, 10900)\n",
      "(10900, 10905)\n",
      "(10905, 10912)\n",
      "(10912, 10917)\n",
      "(10917, 10923)\n",
      "(10923, 10931)\n",
      "(10931, 10936)\n",
      "(10936, 10943)\n",
      "(10943, 10949)\n",
      "(10949, 10954)\n",
      "(10954, 10960)\n",
      "(10960, 10966)\n",
      "(10966, 10972)\n",
      "(10972, 10977)\n",
      "(10977, 10982)\n",
      "(10982, 10988)\n",
      "(10988, 10994)\n",
      "(10994, 11000)\n",
      "(11000, 11006)\n",
      "(11006, 11012)\n",
      "(11012, 11017)\n",
      "(11017, 11023)\n",
      "(11023, 11028)\n",
      "(11028, 11035)\n",
      "(11035, 11044)\n",
      "(11044, 11050)\n",
      "(11050, 11057)\n",
      "(11057, 11064)\n",
      "(11064, 11071)\n",
      "(11071, 11077)\n",
      "(11077, 11082)\n",
      "(11082, 11089)\n",
      "(11089, 11096)\n",
      "(11096, 11103)\n",
      "(11103, 11110)\n",
      "(11110, 11118)\n",
      "(11118, 11126)\n",
      "(11126, 11133)\n",
      "(11133, 11138)\n",
      "(11138, 11144)\n",
      "(11144, 11151)\n",
      "(11151, 11158)\n",
      "(11158, 11165)\n",
      "(11165, 11174)\n",
      "(11174, 11182)\n",
      "(11182, 11189)\n",
      "(11189, 11197)\n",
      "(11197, 11205)\n",
      "(11205, 11213)\n",
      "(11213, 11219)\n",
      "(11219, 11225)\n",
      "(11225, 11231)\n",
      "(11231, 11236)\n",
      "(11236, 11243)\n",
      "(11243, 11248)\n",
      "(11248, 11255)\n",
      "(11255, 11261)\n",
      "(11261, 11266)\n",
      "(11266, 11272)\n",
      "(11272, 11279)\n",
      "(11279, 11285)\n",
      "(11285, 11292)\n",
      "(11292, 11301)\n",
      "(11301, 11307)\n",
      "(11307, 11315)\n",
      "(11315, 11324)\n",
      "(11324, 11331)\n",
      "(11331, 11338)\n",
      "(11338, 11343)\n",
      "(11343, 11348)\n",
      "(11348, 11354)\n",
      "(11354, 11359)\n",
      "(11359, 11366)\n",
      "(11366, 11372)\n",
      "(11372, 11377)\n",
      "(11377, 11383)\n",
      "(11383, 11389)\n",
      "(11389, 11397)\n",
      "(11397, 11402)\n",
      "(11402, 11410)\n",
      "(11410, 11417)\n",
      "(11417, 11423)\n",
      "(11423, 11430)\n",
      "(11430, 11436)\n",
      "(11436, 11443)\n",
      "(11443, 11451)\n",
      "(11451, 11458)\n",
      "(11458, 11465)\n",
      "(11465, 11471)\n",
      "(11471, 11478)\n",
      "(11478, 11486)\n",
      "(11486, 11492)\n",
      "(11492, 11500)\n",
      "(11500, 11507)\n",
      "(11507, 11514)\n",
      "(11514, 11522)\n",
      "(11522, 11531)\n",
      "(11531, 11537)\n",
      "(11537, 11544)\n",
      "(11544, 11549)\n",
      "(11549, 11556)\n",
      "(11556, 11563)\n",
      "(11563, 11570)\n",
      "(11570, 11577)\n",
      "(11577, 11583)\n",
      "(11583, 11588)\n",
      "(11588, 11595)\n",
      "(11595, 11601)\n",
      "(11601, 11607)\n",
      "(11607, 11616)\n",
      "(11616, 11624)\n",
      "(11624, 11630)\n",
      "(11630, 11639)\n",
      "(11639, 11646)\n",
      "(11646, 11652)\n",
      "(11652, 11658)\n",
      "(11658, 11668)\n",
      "(11668, 11676)\n",
      "(11676, 11681)\n",
      "(11681, 11687)\n",
      "(11687, 11697)\n",
      "(11697, 11704)\n",
      "(11704, 11711)\n",
      "(11711, 11716)\n",
      "(11716, 11724)\n",
      "(11724, 11732)\n",
      "(11732, 11740)\n",
      "(11740, 11746)\n",
      "(11746, 11752)\n",
      "(11752, 11760)\n",
      "(11760, 11766)\n",
      "(11766, 11772)\n",
      "(11772, 11779)\n",
      "(11779, 11786)\n",
      "(11786, 11792)\n",
      "(11792, 11798)\n",
      "(11798, 11808)\n",
      "(11808, 11814)\n",
      "(11814, 11819)\n",
      "(11819, 11826)\n",
      "(11826, 11835)\n",
      "(11835, 11840)\n",
      "(11840, 11848)\n",
      "(11848, 11857)\n",
      "(11857, 11865)\n",
      "(11865, 11872)\n",
      "(11872, 11880)\n",
      "(11880, 11886)\n",
      "(11886, 11891)\n",
      "(11891, 11897)\n",
      "(11897, 11902)\n",
      "(11902, 11908)\n",
      "(11908, 11914)\n",
      "(11914, 11924)\n",
      "(11924, 11929)\n",
      "(11929, 11935)\n",
      "(11935, 11940)\n",
      "(11940, 11946)\n",
      "(11946, 11951)\n",
      "(11951, 11958)\n",
      "(11958, 11966)\n",
      "(11966, 11972)\n",
      "(11972, 11978)\n",
      "(11978, 11983)\n",
      "(11983, 11989)\n",
      "(11989, 11994)\n",
      "(11994, 12000)\n",
      "(12000, 12007)\n",
      "(12007, 12013)\n",
      "(12013, 12018)\n",
      "(12018, 12024)\n",
      "(12024, 12031)\n",
      "(12031, 12036)\n",
      "(12036, 12042)\n",
      "(12042, 12049)\n",
      "(12049, 12055)\n",
      "(12055, 12061)\n",
      "(12061, 12068)\n",
      "(12068, 12074)\n",
      "(12074, 12083)\n",
      "(12083, 12090)\n",
      "(12090, 12098)\n",
      "(12098, 12105)\n",
      "(12105, 12112)\n",
      "(12112, 12119)\n",
      "(12119, 12125)\n",
      "(12125, 12131)\n",
      "(12131, 12139)\n",
      "(12139, 12146)\n",
      "(12146, 12152)\n",
      "(12152, 12157)\n",
      "(12157, 12163)\n",
      "(12163, 12170)\n",
      "(12170, 12177)\n",
      "(12177, 12183)\n",
      "(12183, 12188)\n",
      "(12188, 12194)\n",
      "(12194, 12200)\n",
      "(12200, 12206)\n",
      "(12206, 12212)\n",
      "(12212, 12220)\n",
      "(12220, 12228)\n",
      "(12228, 12235)\n",
      "(12235, 12241)\n",
      "(12241, 12247)\n",
      "(12247, 12256)\n",
      "(12256, 12263)\n",
      "(12263, 12269)\n",
      "(12269, 12276)\n",
      "(12276, 12283)\n",
      "(12283, 12289)\n",
      "(12289, 12296)\n",
      "(12296, 12303)\n",
      "(12303, 12310)\n",
      "(12310, 12316)\n",
      "(12316, 12324)\n",
      "(12324, 12331)\n",
      "(12331, 12339)\n",
      "(12339, 12345)\n",
      "(12345, 12353)\n",
      "(12353, 12360)\n",
      "(12360, 12367)\n",
      "(12367, 12374)\n",
      "(12374, 12379)\n",
      "(12379, 12386)\n",
      "(12386, 12391)\n",
      "(12391, 12398)\n",
      "(12398, 12403)\n",
      "(12403, 12410)\n",
      "(12410, 12416)\n",
      "(12416, 12423)\n",
      "(12423, 12431)\n",
      "(12431, 12438)\n",
      "(12438, 12446)\n",
      "(12446, 12452)\n",
      "(12452, 12457)\n",
      "(12457, 12463)\n",
      "(12463, 12468)\n",
      "(12468, 12478)\n",
      "(12478, 12484)\n",
      "(12484, 12490)\n",
      "(12490, 12498)\n",
      "(12498, 12506)\n",
      "(12506, 12513)\n",
      "(12513, 12520)\n",
      "(12520, 12527)\n",
      "(12527, 12534)\n",
      "(12534, 12541)\n",
      "(12541, 12546)\n",
      "(12546, 12552)\n",
      "(12552, 12559)\n",
      "(12559, 12564)\n",
      "(12564, 12569)\n",
      "(12569, 12574)\n",
      "(12574, 12581)\n",
      "(12581, 12586)\n",
      "(12586, 12592)\n",
      "(12592, 12597)\n",
      "(12597, 12602)\n",
      "(12602, 12611)\n",
      "(12611, 12618)\n",
      "(12618, 12625)\n",
      "(12625, 12631)\n",
      "(12631, 12639)\n",
      "(12639, 12646)\n",
      "(12646, 12652)\n",
      "(12652, 12658)\n",
      "(12658, 12664)\n",
      "(12664, 12669)\n",
      "(12669, 12676)\n",
      "(12676, 12682)\n"
     ]
    }
   ],
   "source": [
    "start = end = 10020\n",
    "for i in range( len(test_len) ) :\n",
    "    end += test_len[i]\n",
    "    print (start, end)\n",
    "    test_i = Total_Final[start:end]\n",
    "    test_i.to_csv('../MODEL/SVM_Data/TEST/' + 'test_' + str(i) + '.csv', encoding='utf-8', index = False)\n",
    "    start += test_len[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
